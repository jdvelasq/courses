

<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Copyright 2020 The TensorFlow Authors. &mdash; documentaci√≥n de --- Cursos --- - </title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../_static/language_data.js"></script>
        <script type="text/javascript" src="../../../_static/clipboard.min.js"></script>
        <script type="text/javascript" src="../../../_static/copybutton.js"></script>
        <script type="text/javascript" src="../../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="√çndice" href="../../../genindex.html" />
    <link rel="search" title="B√∫squeda" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> --- Cursos ---
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Configuraci√≥n</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../setup.html">Instalaci√≥n de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Pregrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../fundamentos-de-analitica/index.html">Fundamentos de Anal√≠tica</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../redes_neuronales_y_algoritmos_bioinspirados/index.html">Redes Neuronales Artificiales y Algoritmos Bioinspirados</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Posgrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-de-grandes-datos/index.html">Anal√≠tica de Grandes Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../analitica-predictiva/index.html">Anal√≠tica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ciencia-de-los-datos/index.html">Ciencia de los Datos Aplicada</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../productos-de-datos/index.html">Productos de Datos</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">--- Cursos ---</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Copyright 2020 The TensorFlow Authors.</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../_sources/notebooks/tensorflow/guide/tensor.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Copyright-2020-The-TensorFlow-Authors.">
<h1>Copyright 2020 The TensorFlow Authors.<a class="headerlink" href="#Copyright-2020-The-TensorFlow-Authors." title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h1>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>#@title Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
</pre></div>
</div>
</div>
<div class="section" id="Introduction-to-Tensors">
<h2>Introduction to Tensors<a class="headerlink" href="#Introduction-to-Tensors" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h2>
<table class="tfo-notebook-buttons" align="left"><td><p><a href="#id1"><span class="problematic" id="id2">|</span></a>54a10a59eebf462fa31ebb15cf295b56|View on TensorFlow.org</p>
</td><td><p><a href="#id3"><span class="problematic" id="id4">|</span></a>de8b6ed3407f4436b0fc2a6a512937fa|Run in Google Colab</p>
</td><td><p><a href="#id5"><span class="problematic" id="id6">|</span></a>84091aeeb5dd41cb98da247c0cc02fe3|View source on GitHub</p>
</td><td><p><a href="#id7"><span class="problematic" id="id8">|</span></a>1adb368247f34a3d87fa16eef3a23328|Download notebook</p>
</td></table><div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import tensorflow as tf
import numpy as np
</pre></div>
</div>
</div>
<p>Tensors are multi-dimensional arrays with a uniform type (called a <code class="docutils literal notranslate"><span class="pre">dtype</span></code>). You can see all supported <code class="docutils literal notranslate"><span class="pre">dtypes</span></code> at <code class="docutils literal notranslate"><span class="pre">tf.dtypes.DType</span></code>.</p>
<p>If you‚Äôre familiar with <a class="reference external" href="https://numpy.org/devdocs/user/quickstart.html">NumPy</a>, tensors are (kind of) like <code class="docutils literal notranslate"><span class="pre">np.arrays</span></code>.</p>
<p>All tensors are immutable like Python numbers and strings: you can never update the contents of a tensor, only create a new one.</p>
<div class="section" id="Basics">
<h3>Basics<a class="headerlink" href="#Basics" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h3>
<p>Let‚Äôs create some basic tensors.</p>
<p>Here is a ‚Äúscalar‚Äù or ‚Äúrank-0‚Äù tensor . A scalar contains a single value, and no ‚Äúaxes‚Äù.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># This will be an int32 tensor by default; see &quot;dtypes&quot; below.
rank_0_tensor = tf.constant(4)
print(rank_0_tensor)
</pre></div>
</div>
</div>
<p>A ‚Äúvector‚Äù or ‚Äúrank-1‚Äù tensor is like a list of values. A vector has one axis:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Let&#39;s make this a float tensor.
rank_1_tensor = tf.constant([2.0, 3.0, 4.0])
print(rank_1_tensor)
</pre></div>
</div>
</div>
<p>A ‚Äúmatrix‚Äù or ‚Äúrank-2‚Äù tensor has two axes:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># If you want to be specific, you can set the dtype (see below) at creation time
rank_2_tensor = tf.constant([[1, 2],
                             [3, 4],
                             [5, 6]], dtype=tf.float16)
print(rank_2_tensor)
</pre></div>
</div>
</div>
<table><tr><th><p>A scalar, shape: []</p>
</th><th><p>A vector, shape: [3]</p>
</th><th><p>A matrix, shape: [3, 2]</p>
</th></tr><tr><td><p><img alt="A scalar, the number 4" src="../../../_images/scalar.png" /></p>
</td><td><p><img alt="The line with 3 sections, each one containing a number." src="../../../_images/vector.png" /></p>
</td><td><p><img alt="A 3x2 grid, with each cell containing a number." src="../../../_images/matrix.png" /></p>
</td></tr></table><p>Tensors may have more axes; here is a tensor with three axes:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># There can be an arbitrary number of
# axes (sometimes called &quot;dimensions&quot;)
rank_3_tensor = tf.constant([
  [[0, 1, 2, 3, 4],
   [5, 6, 7, 8, 9]],
  [[10, 11, 12, 13, 14],
   [15, 16, 17, 18, 19]],
  [[20, 21, 22, 23, 24],
   [25, 26, 27, 28, 29]],])

print(rank_3_tensor)
</pre></div>
</div>
</div>
<p>There are many ways you might visualize a tensor with more than two axes.</p>
<table><tr><th colspan="3"><p>A 3-axis tensor, shape: [3, 2, 5]</p>
</th><tr><tr><td><p><img alt="b52afbd65b754a32a415ecfd69cad4dc" src="../../../_images/3-axis_numpy.png" /></p>
</td><td><p><img alt="3951a0248c8f41a3bb2e927b4f2fcc2d" src="../../../_images/3-axis_front.png" /></p>
</td><td><p><img alt="b361967a567540dea830e8ff1d7328e7" src="../../../_images/3-axis_block.png" /></p>
</td></tr></table><p>You can convert a tensor to a NumPy array either using <code class="docutils literal notranslate"><span class="pre">np.array</span></code> or the <code class="docutils literal notranslate"><span class="pre">tensor.numpy</span></code> method:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>np.array(rank_2_tensor)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>rank_2_tensor.numpy()
</pre></div>
</div>
</div>
<p>Tensors often contain floats and ints, but have many other types, including:</p>
<ul class="simple">
<li><p>complex numbers</p></li>
<li><p>strings</p></li>
</ul>
<p>The base <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code> class requires tensors to be ‚Äúrectangular‚Äù‚Äîthat is, along each axis, every element is the same size. However, there are specialized types of tensors that can handle different shapes:</p>
<ul class="simple">
<li><p>Ragged tensors (see <a class="reference external" href="#ragged_tensors">RaggedTensor</a> below)</p></li>
<li><p>Sparse tensors (see <a class="reference external" href="#sparse_tensors">SparseTensor</a> below)</p></li>
</ul>
<p>You can do basic math on tensors, including addition, element-wise multiplication, and matrix multiplication.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>a = tf.constant([[1, 2],
                 [3, 4]])
b = tf.constant([[1, 1],
                 [1, 1]]) # Could have also said `tf.ones([2,2])`

print(tf.add(a, b), &quot;\n&quot;)
print(tf.multiply(a, b), &quot;\n&quot;)
print(tf.matmul(a, b), &quot;\n&quot;)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(a + b, &quot;\n&quot;) # element-wise addition
print(a * b, &quot;\n&quot;) # element-wise multiplication
print(a @ b, &quot;\n&quot;) # matrix multiplication
</pre></div>
</div>
</div>
<p>Tensors are used in all kinds of operations (ops).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>c = tf.constant([[4.0, 5.0], [10.0, 1.0]])

# Find the largest value
print(tf.reduce_max(c))
# Find the index of the largest value
print(tf.argmax(c))
# Compute the softmax
print(tf.nn.softmax(c))
</pre></div>
</div>
</div>
</div>
<div class="section" id="About-shapes">
<h3>About shapes<a class="headerlink" href="#About-shapes" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h3>
<p>Tensors have shapes. Some vocabulary:</p>
<ul class="simple">
<li><p><strong>Shape</strong>: The length (number of elements) of each of the axes of a tensor.</p></li>
<li><p><strong>Rank</strong>: Number of tensor axes. A scalar has rank 0, a vector has rank 1, a matrix is rank 2.</p></li>
<li><p><strong>Axis</strong> or <strong>Dimension</strong>: A particular dimension of a tensor.</p></li>
<li><p><strong>Size</strong>: The total number of items in the tensor, the product shape vector.</p></li>
</ul>
<p>Note: Although you may see reference to a ‚Äútensor of two dimensions‚Äù, a rank-2 tensor does not usually describe a 2D space.</p>
<p>Tensors and <code class="docutils literal notranslate"><span class="pre">tf.TensorShape</span></code> objects have convenient properties for accessing these:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>rank_4_tensor = tf.zeros([3, 2, 4, 5])
</pre></div>
</div>
</div>
<table><tr><th colspan="2"><p>A rank-4 tensor, shape: [3, 2, 4, 5]</p>
</th></tr><tr><td><p><img alt="A tensor shape is like a vector." src="../../../_images/shape.png" /></p>
<td><p><img alt="A 4-axis tensor" src="../../../_images/4-axis_block.png" /></p>
</td></tr></table><div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(&quot;Type of every element:&quot;, rank_4_tensor.dtype)
print(&quot;Number of axes:&quot;, rank_4_tensor.ndim)
print(&quot;Shape of tensor:&quot;, rank_4_tensor.shape)
print(&quot;Elements along axis 0 of tensor:&quot;, rank_4_tensor.shape[0])
print(&quot;Elements along the last axis of tensor:&quot;, rank_4_tensor.shape[-1])
print(&quot;Total number of elements (3*2*4*5): &quot;, tf.size(rank_4_tensor).numpy())
</pre></div>
</div>
</div>
<p>While axes are often referred to by their indices, you should always keep track of the meaning of each. Often axes are ordered from global to local: The batch axis first, followed by spatial dimensions, and features for each location last. This way feature vectors are contiguous regions of memory.</p>
<table><tr><th><p>Typical axis order</p>
</th></tr><tr><td><p><img alt="Keep track of what each axis is. A 4-axis tensor might be: Batch, Width, Height, Features" src="../../../_images/shape2.png" /></p>
</td></tr></table></div>
<div class="section" id="Indexing">
<h3>Indexing<a class="headerlink" href="#Indexing" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h3>
<div class="section" id="Single-axis-indexing">
<h4>Single-axis indexing<a class="headerlink" href="#Single-axis-indexing" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h4>
<p>TensorFlow follows standard Python indexing rules, similar to <a class="reference external" href="https://docs.python.org/3/tutorial/introduction.html#strings">indexing a list or a string in Python</a>, and the basic rules for NumPy indexing.</p>
<ul class="simple">
<li><p>indexes start at <code class="docutils literal notranslate"><span class="pre">0</span></code></p></li>
<li><p>negative indices count backwards from the end</p></li>
<li><p>colons, <code class="docutils literal notranslate"><span class="pre">:</span></code>, are used for slices: <code class="docutils literal notranslate"><span class="pre">start:stop:step</span></code></p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>rank_1_tensor = tf.constant([0, 1, 1, 2, 3, 5, 8, 13, 21, 34])
print(rank_1_tensor.numpy())
</pre></div>
</div>
</div>
<p>Indexing with a scalar removes the axis:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(&quot;First:&quot;, rank_1_tensor[0].numpy())
print(&quot;Second:&quot;, rank_1_tensor[1].numpy())
print(&quot;Last:&quot;, rank_1_tensor[-1].numpy())
</pre></div>
</div>
</div>
<p>Indexing with a <code class="docutils literal notranslate"><span class="pre">:</span></code> slice keeps the axis:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(&quot;Everything:&quot;, rank_1_tensor[:].numpy())
print(&quot;Before 4:&quot;, rank_1_tensor[:4].numpy())
print(&quot;From 4 to the end:&quot;, rank_1_tensor[4:].numpy())
print(&quot;From 2, before 7:&quot;, rank_1_tensor[2:7].numpy())
print(&quot;Every other item:&quot;, rank_1_tensor[::2].numpy())
print(&quot;Reversed:&quot;, rank_1_tensor[::-1].numpy())
</pre></div>
</div>
</div>
</div>
<div class="section" id="Multi-axis-indexing">
<h4>Multi-axis indexing<a class="headerlink" href="#Multi-axis-indexing" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h4>
<p>Higher rank tensors are indexed by passing multiple indices.</p>
<p>The exact same rules as in the single-axis case apply to each axis independently.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(rank_2_tensor.numpy())
</pre></div>
</div>
</div>
<p>Passing an integer for each index, the result is a scalar.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Pull out a single value from a 2-rank tensor
print(rank_2_tensor[1, 1].numpy())
</pre></div>
</div>
</div>
<p>You can index using any combination of integers and slices:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Get row and column tensors
print(&quot;Second row:&quot;, rank_2_tensor[1, :].numpy())
print(&quot;Second column:&quot;, rank_2_tensor[:, 1].numpy())
print(&quot;Last row:&quot;, rank_2_tensor[-1, :].numpy())
print(&quot;First item in last column:&quot;, rank_2_tensor[0, -1].numpy())
print(&quot;Skip the first row:&quot;)
print(rank_2_tensor[1:, :].numpy(), &quot;\n&quot;)
</pre></div>
</div>
</div>
<p>Here is an example with a 3-axis tensor:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(rank_3_tensor[:, :, 4])
</pre></div>
</div>
</div>
<table><tr><th colspan="2"><p>Selecting the last feature across all locations in each example in the batch</p>
</th></tr><tr><td><p><img alt="A 3x2x5 tensor with all the values at the index-4 of the last axis selected." src="../../../_images/index1.png" /></p>
</td><td><p><img alt="The selected values packed into a 2-axis tensor." src="../../../_images/index2.png" /></p>
</td></tr></table><p>Read the <a class="reference external" href="https://tensorflow.org/guide/tensor_slicing">tensor slicing guide</a> to learn how you can apply indexing to manipulate individual elements in your tensors.</p>
</div>
</div>
<div class="section" id="Manipulating-Shapes">
<h3>Manipulating Shapes<a class="headerlink" href="#Manipulating-Shapes" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h3>
<p>Reshaping a tensor is of great utility.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Shape returns a `TensorShape` object that shows the size along each axis
x = tf.constant([[1], [2], [3]])
print(x.shape)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># You can convert this object into a Python list, too
print(x.shape.as_list())
</pre></div>
</div>
</div>
<p>You can reshape a tensor into a new shape. The <code class="docutils literal notranslate"><span class="pre">tf.reshape</span></code> operation is fast and cheap as the underlying data does not need to be duplicated.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># You can reshape a tensor to a new shape.
# Note that you&#39;re passing in a list
reshaped = tf.reshape(x, [1, 3])
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(x.shape)
print(reshaped.shape)
</pre></div>
</div>
</div>
<p>The data maintains its layout in memory and a new tensor is created, with the requested shape, pointing to the same data. TensorFlow uses C-style ‚Äúrow-major‚Äù memory ordering, where incrementing the rightmost index corresponds to a single step in memory.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(rank_3_tensor)
</pre></div>
</div>
</div>
<p>If you flatten a tensor you can see what order it is laid out in memory.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># A `-1` passed in the `shape` argument says &quot;Whatever fits&quot;.
print(tf.reshape(rank_3_tensor, [-1]))
</pre></div>
</div>
</div>
<p>Typically the only reasonable use of <code class="docutils literal notranslate"><span class="pre">tf.reshape</span></code> is to combine or split adjacent axes (or add/remove <code class="docutils literal notranslate"><span class="pre">1</span></code>s).</p>
<p>For this 3x2x5 tensor, reshaping to (3x2)x5 or 3x(2x5) are both reasonable things to do, as the slices do not mix:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(tf.reshape(rank_3_tensor, [3*2, 5]), &quot;\n&quot;)
print(tf.reshape(rank_3_tensor, [3, -1]))
</pre></div>
</div>
</div>
<table><th colspan="3"><p>Some good reshapes.</p>
</th><tr><td><p><img alt="A 3x2x5 tensor" src="../../../_images/reshape-before.png" /></p>
</td><td><p><img alt="The same data reshaped to (3x2)x5" src="../../../_images/reshape-good1.png" /></p>
</td><td><p><img alt="The same data reshaped to 3x(2x5)" src="../../../_images/reshape-good2.png" /></p>
</td></tr></table><p>Reshaping will ‚Äúwork‚Äù for any new shape with the same total number of elements, but it will not do anything useful if you do not respect the order of the axes.</p>
<p>Swapping axes in <code class="docutils literal notranslate"><span class="pre">tf.reshape</span></code> does not work; you need <code class="docutils literal notranslate"><span class="pre">tf.transpose</span></code> for that.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Bad examples: don&#39;t do this

# You can&#39;t reorder axes with reshape.
print(tf.reshape(rank_3_tensor, [2, 3, 5]), &quot;\n&quot;)

# This is a mess
print(tf.reshape(rank_3_tensor, [5, 6]), &quot;\n&quot;)

# This doesn&#39;t work at all
try:
  tf.reshape(rank_3_tensor, [7, -1])
except Exception as e:
  print(f&quot;{type(e).__name__}: {e}&quot;)
</pre></div>
</div>
</div>
<table><th colspan="3"><p>Some bad reshapes.</p>
</th><tr><td><p><img alt="You can't reorder axes, use tf.transpose for that" src="../../../_images/reshape-bad.png" /></p>
</td><td><p><img alt="Anything that mixes the slices of data together is probably wrong." src="../../../_images/reshape-bad4.png" /></p>
</td><td><p><img alt="The new shape must fit exactly." src="../../../_images/reshape-bad2.png" /></p>
</td></tr></table><p>You may run across not-fully-specified shapes. Either the shape contains a <code class="docutils literal notranslate"><span class="pre">None</span></code> (an axis-length is unknown) or the whole shape is <code class="docutils literal notranslate"><span class="pre">None</span></code> (the rank of the tensor is unknown).</p>
<p>Except for <a class="reference external" href="#ragged_tensors">tf.RaggedTensor</a>, such shapes will only occur in the context of TensorFlow‚Äôs symbolic, graph-building APIs:</p>
<ul class="simple">
<li><p><a class="reference internal" href="function.html"><span class="doc">tf.function</span></a></p></li>
<li><p>The <a class="reference external" href="keras/functional.ipynb">keras functional API</a>.</p></li>
</ul>
</div>
<div class="section" id="More-on-DTypes">
<h3>More on <code class="docutils literal notranslate"><span class="pre">DTypes</span></code><a class="headerlink" href="#More-on-DTypes" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h3>
<p>To inspect a <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code>‚Äôs data type use the <code class="docutils literal notranslate"><span class="pre">Tensor.dtype</span></code> property.</p>
<p>When creating a <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code> from a Python object you may optionally specify the datatype.</p>
<p>If you don‚Äôt, TensorFlow chooses a datatype that can represent your data. TensorFlow converts Python integers to <code class="docutils literal notranslate"><span class="pre">tf.int32</span></code> and Python floating point numbers to <code class="docutils literal notranslate"><span class="pre">tf.float32</span></code>. Otherwise TensorFlow uses the same rules NumPy uses when converting to arrays.</p>
<p>You can cast from type to type.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>the_f64_tensor = tf.constant([2.2, 3.3, 4.4], dtype=tf.float64)
the_f16_tensor = tf.cast(the_f64_tensor, dtype=tf.float16)
# Now, cast to an uint8 and lose the decimal precision
the_u8_tensor = tf.cast(the_f16_tensor, dtype=tf.uint8)
print(the_u8_tensor)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Broadcasting">
<h3>Broadcasting<a class="headerlink" href="#Broadcasting" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h3>
<p>Broadcasting is a concept borrowed from the <a class="reference external" href="https://numpy.org/doc/stable/user/basics.html">equivalent feature in NumPy</a>. In short, under certain conditions, smaller tensors are ‚Äústretched‚Äù automatically to fit larger tensors when running combined operations on them.</p>
<p>The simplest and most common case is when you attempt to multiply or add a tensor to a scalar. In that case, the scalar is broadcast to be the same shape as the other argument.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.constant([1, 2, 3])

y = tf.constant(2)
z = tf.constant([2, 2, 2])
# All of these are the same computation
print(tf.multiply(x, 2))
print(x * y)
print(x * z)
</pre></div>
</div>
</div>
<p>Likewise, axes with length 1 can be stretched out to match the other arguments. Both arguments can be stretched in the same computation.</p>
<p>In this case a 3x1 matrix is element-wise multiplied by a 1x4 matrix to produce a 3x4 matrix. Note how the leading 1 is optional: The shape of y is <code class="docutils literal notranslate"><span class="pre">[4]</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># These are the same computations
x = tf.reshape(x,[3,1])
y = tf.range(1, 5)
print(x, &quot;\n&quot;)
print(y, &quot;\n&quot;)
print(tf.multiply(x, y))
</pre></div>
</div>
</div>
<table><tr><th><p>A broadcasted add: a [3, 1] times a [1, 4] gives a [3,4]</p>
</th></tr><tr><td><p><img alt="Adding a 3x1 matrix to a 4x1 matrix results in a 3x4 matrix" src="../../../_images/broadcasting.png" /></p>
</td></tr></table><p>Here is the same operation without broadcasting:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x_stretch = tf.constant([[1, 1, 1, 1],
                         [2, 2, 2, 2],
                         [3, 3, 3, 3]])

y_stretch = tf.constant([[1, 2, 3, 4],
                         [1, 2, 3, 4],
                         [1, 2, 3, 4]])

print(x_stretch * y_stretch)  # Again, operator overloading
</pre></div>
</div>
</div>
<p>Most of the time, broadcasting is both time and space efficient, as the broadcast operation never materializes the expanded tensors in memory.</p>
<p>You see what broadcasting looks like using <code class="docutils literal notranslate"><span class="pre">tf.broadcast_to</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(tf.broadcast_to(tf.constant([1, 2, 3]), [3, 3]))
</pre></div>
</div>
</div>
<p>Unlike a mathematical op, for example, <code class="docutils literal notranslate"><span class="pre">broadcast_to</span></code> does nothing special to save memory. Here, you are materializing the tensor.</p>
<p>It can get even more complicated. <a class="reference external" href="https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html">This section</a> of Jake VanderPlas‚Äôs book <em>Python Data Science Handbook</em> shows more broadcasting tricks (again in NumPy).</p>
</div>
<div class="section" id="tf.convert_to_tensor">
<h3>tf.convert_to_tensor<a class="headerlink" href="#tf.convert_to_tensor" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h3>
<p>Most ops, like <code class="docutils literal notranslate"><span class="pre">tf.matmul</span></code> and <code class="docutils literal notranslate"><span class="pre">tf.reshape</span></code> take arguments of class <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code>. However, you‚Äôll notice in the above case, Python objects shaped like tensors are accepted.</p>
<p>Most, but not all, ops call <code class="docutils literal notranslate"><span class="pre">convert_to_tensor</span></code> on non-tensor arguments. There is a registry of conversions, and most object classes like NumPy‚Äôs <code class="docutils literal notranslate"><span class="pre">ndarray</span></code>, <code class="docutils literal notranslate"><span class="pre">TensorShape</span></code>, Python lists, and <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code> will all convert automatically.</p>
<p>See <code class="docutils literal notranslate"><span class="pre">tf.register_tensor_conversion_function</span></code> for more details, and if you have your own type you‚Äôd like to automatically convert to a tensor.</p>
</div>
<div class="section" id="Ragged-Tensors">
<h3>Ragged Tensors<a class="headerlink" href="#Ragged-Tensors" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h3>
<p>A tensor with variable numbers of elements along some axis is called ‚Äúragged‚Äù. Use <code class="docutils literal notranslate"><span class="pre">tf.ragged.RaggedTensor</span></code> for ragged data.</p>
<p>For example, This cannot be represented as a regular tensor:</p>
<table><tr><th><p>A <code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor</span></code>, shape: [4, None]</p>
</th></tr><tr><td><p><img alt="A 2-axis ragged tensor, each row can have a different length." src="../../../_images/ragged.png" /></p>
</td></tr></table><div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>ragged_list = [
    [0, 1, 2, 3],
    [4, 5],
    [6, 7, 8],
    [9]]
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>try:
  tensor = tf.constant(ragged_list)
except Exception as e:
  print(f&quot;{type(e).__name__}: {e}&quot;)
</pre></div>
</div>
</div>
<p>Instead create a <code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor</span></code> using <code class="docutils literal notranslate"><span class="pre">tf.ragged.constant</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>ragged_tensor = tf.ragged.constant(ragged_list)
print(ragged_tensor)
</pre></div>
</div>
</div>
<p>The shape of a <code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor</span></code> will contain some axes with unknown lengths:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(ragged_tensor.shape)
</pre></div>
</div>
</div>
</div>
<div class="section" id="String-tensors">
<h3>String tensors<a class="headerlink" href="#String-tensors" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">tf.string</span></code> is a <code class="docutils literal notranslate"><span class="pre">dtype</span></code>, which is to say you can represent data as strings (variable-length byte arrays) in tensors.</p>
<p>The strings are atomic and cannot be indexed the way Python strings are. The length of the string is not one of the axes of the tensor. See <code class="docutils literal notranslate"><span class="pre">tf.strings</span></code> for functions to manipulate them.</p>
<p>Here is a scalar string tensor:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Tensors can be strings, too here is a scalar string.
scalar_string_tensor = tf.constant(&quot;Gray wolf&quot;)
print(scalar_string_tensor)
</pre></div>
</div>
</div>
<p>And a vector of strings:</p>
<table><tr><th><p>A vector of strings, shape: [3,]</p>
</th></tr><tr><td><p><img alt="The string length is not one of the tensor's axes." src="../../../_images/strings.png" /></p>
</td></tr></table><div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># If you have three string tensors of different lengths, this is OK.
tensor_of_strings = tf.constant([&quot;Gray wolf&quot;,
                                 &quot;Quick brown fox&quot;,
                                 &quot;Lazy dog&quot;])
# Note that the shape is (3,). The string length is not included.
print(tensor_of_strings)
</pre></div>
</div>
</div>
<p>In the above printout the <code class="docutils literal notranslate"><span class="pre">b</span></code> prefix indicates that <code class="docutils literal notranslate"><span class="pre">tf.string</span></code> dtype is not a unicode string, but a byte-string. See the <a class="reference external" href="https://www.tensorflow.org/tutorials/load_data/unicode">Unicode Tutorial</a> for more about working with unicode text in TensorFlow.</p>
<p>If you pass unicode characters they are utf-8 encoded.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>tf.constant(&quot;ü•≥üëç&quot;)
</pre></div>
</div>
</div>
<p>Some basic functions with strings can be found in <code class="docutils literal notranslate"><span class="pre">tf.strings</span></code>, including <code class="docutils literal notranslate"><span class="pre">tf.strings.split</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># You can use split to split a string into a set of tensors
print(tf.strings.split(scalar_string_tensor, sep=&quot; &quot;))
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># ...but it turns into a `RaggedTensor` if you split up a tensor of strings,
# as each string might be split into a different number of parts.
print(tf.strings.split(tensor_of_strings))
</pre></div>
</div>
</div>
<table><tr><th><p>Three strings split, shape: [3, None]</p>
</th></tr><tr><td><p><img alt="Splitting multiple strings returns a tf.RaggedTensor" src="../../../_images/string-split.png" /></p>
</td></tr></table><p>And <code class="docutils literal notranslate"><span class="pre">tf.string.to_number</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>text = tf.constant(&quot;1 10 100&quot;)
print(tf.strings.to_number(tf.strings.split(text, &quot; &quot;)))
</pre></div>
</div>
</div>
<p>Although you can‚Äôt use <code class="docutils literal notranslate"><span class="pre">tf.cast</span></code> to turn a string tensor into numbers, you can convert it into bytes, and then into numbers.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>byte_strings = tf.strings.bytes_split(tf.constant(&quot;Duck&quot;))
byte_ints = tf.io.decode_raw(tf.constant(&quot;Duck&quot;), tf.uint8)
print(&quot;Byte strings:&quot;, byte_strings)
print(&quot;Bytes:&quot;, byte_ints)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Or split it up as unicode and then decode it
unicode_bytes = tf.constant(&quot;„Ç¢„Éí„É´ ü¶Ü&quot;)
unicode_char_bytes = tf.strings.unicode_split(unicode_bytes, &quot;UTF-8&quot;)
unicode_values = tf.strings.unicode_decode(unicode_bytes, &quot;UTF-8&quot;)

print(&quot;\nUnicode bytes:&quot;, unicode_bytes)
print(&quot;\nUnicode chars:&quot;, unicode_char_bytes)
print(&quot;\nUnicode values:&quot;, unicode_values)
</pre></div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">tf.string</span></code> dtype is used for all raw bytes data in TensorFlow. The <code class="docutils literal notranslate"><span class="pre">tf.io</span></code> module contains functions for converting data to and from bytes, including decoding images and parsing csv.</p>
</div>
<div class="section" id="Sparse-tensors">
<h3>Sparse tensors<a class="headerlink" href="#Sparse-tensors" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h3>
<p>Sometimes, your data is sparse, like a very wide embedding space. TensorFlow supports <code class="docutils literal notranslate"><span class="pre">tf.sparse.SparseTensor</span></code> and related operations to store sparse data efficiently.</p>
<table><tr><th><p>A <code class="docutils literal notranslate"><span class="pre">tf.SparseTensor</span></code>, shape: [3, 4]</p>
</th></tr><tr><td><p><img alt="An 3x4 grid, with values in only two of the cells." src="../../../_images/sparse.png" /></p>
</td></tr></table><div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Sparse tensors store values by index in a memory-efficient manner
sparse_tensor = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]],
                                       values=[1, 2],
                                       dense_shape=[3, 4])
print(sparse_tensor, &quot;\n&quot;)

# You can convert sparse tensors to dense
print(tf.sparse.to_dense(sparse_tensor))
</pre></div>
</div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Derechos de autor 2019, Juan D. Velasquez.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>