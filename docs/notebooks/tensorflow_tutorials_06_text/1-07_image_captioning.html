<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Image captioning with visual attention &mdash; documentación de Cursos de Analítica y Machine Learning - </title><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script src="../../_static/clipboard.min.js"></script>
        <script src="../../_static/copybutton.js"></script>
        <script src="../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Índice" href="../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../search.html" />
    <link rel="next" title="Clasificación de datos estructurados usando capas de preprocesamiento — 0:00 min" href="../tensorflow_tutorials_08_structured_data/1-01_classify_structured_data_with_preprocessing_layers.html" />
    <link rel="prev" title="&lt;no title&gt;" href="1-06_neural_machine_translation_with_attention.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> Cursos de Analítica y Machine Learning
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Buscar documentos" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Pregrado</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html">Redes Neuronales Artificiales y Aprendizaje Profundo</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-01-2022-mar-08">Sesión 01 — 2022-mar-08</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-02-2022-mar-14">Sesión 02 — 2022-mar-14</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-03-2022-mar-22">Sesión 03 — 2022-mar-22</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-04-2022-mar-29">Sesión 04 — 2022-mar-29</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-05-2022-abr-05">Sesión 05 — 2022-abr-05</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-06-2022-abr-19">Sesión 06 — 2022-abr-19</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-07-2022-abr-26">Sesión 07 — 2022-abr-26,</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-08-2022-may-03">Sesión 08 — 2022-may-03</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-09-2022-may-10">Sesión 09 — 2022-may-10</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-10-2022-may-17">Sesión 10 — 2022-may-17</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-11-2022-may-24">Sesión 11 — 2022-may-24</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-12-2022-may-31">Sesión 12 — 2022-may-31</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-13-2022-jun-07">Sesión 13 — 2022-jun-07</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-14-2022-jun-14">Sesión 14 — 2022-jun-14</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-15-2022-jun-21">Sesión 15 — 2022-jun-21</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-16-2022-jun-28">Sesión 16 — 2022-jun-28</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#material-para-proximos-cursos-2023">Material para próximos cursos (2023)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../fundamentos_de_analitica/index.html">Fundamentos de Analítica</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Posgrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../analitica_de_grandes_datos/index.html">Analítica de Grandes Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../analitica_predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ciencia_de_los_datos/index.html">Ciencia de los Datos Aplicada</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../productos_de_datos/index.html">Productos de Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../analitica_avanzada/index.html">Analítica Avanzada</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Cursos de Analítica y Machine Learning</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html">Redes Neuronales Artificiales y Aprendizaje Profundo</a> &raquo;</li>
      <li>Image captioning with visual attention</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/notebooks/tensorflow_tutorials_06_text/1-07_image_captioning.ipynb.txt" rel="nofollow"> Ver código fuente de la página</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Image-captioning-with-visual-attention">
<h1>Image captioning with visual attention<a class="headerlink" href="#Image-captioning-with-visual-attention" title="Enlazar permanentemente con este título"></a></h1>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># You&#39;ll generate plots of attention in order to see which parts of an image</span>
<span class="c1"># your model focuses on during captioning</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
</pre></div>
</div>
</div>
<div class="section" id="Download-and-prepare-the-MS-COCO-dataset">
<h2>Download and prepare the MS-COCO dataset<a class="headerlink" href="#Download-and-prepare-the-MS-COCO-dataset" title="Enlazar permanentemente con este título"></a></h2>
<p>You will use the <a class="reference external" href="http://cocodataset.org/#home">MS-COCO dataset</a> to train your model. The dataset contains over 82,000 images, each of which has at least 5 different caption annotations. The code below downloads and extracts the dataset automatically.</p>
<p><strong>Caution: large download ahead</strong>. You’ll use the training set, which is a 13GB file.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Download caption annotation files</span>
<span class="n">annotation_folder</span> <span class="o">=</span> <span class="s1">&#39;/annotations/&#39;</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)</span> <span class="o">+</span> <span class="n">annotation_folder</span><span class="p">):</span>
  <span class="n">annotation_zip</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">get_file</span><span class="p">(</span><span class="s1">&#39;captions.zip&#39;</span><span class="p">,</span>
                                           <span class="n">cache_subdir</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">),</span>
                                           <span class="n">origin</span><span class="o">=</span><span class="s1">&#39;http://images.cocodataset.org/annotations/annotations_trainval2014.zip&#39;</span><span class="p">,</span>
                                           <span class="n">extract</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">annotation_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">annotation_zip</span><span class="p">)</span><span class="o">+</span><span class="s1">&#39;/annotations/captions_train2014.json&#39;</span>
  <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">annotation_zip</span><span class="p">)</span>

<span class="c1"># Download image files</span>
<span class="n">image_folder</span> <span class="o">=</span> <span class="s1">&#39;/train2014/&#39;</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)</span> <span class="o">+</span> <span class="n">image_folder</span><span class="p">):</span>
  <span class="n">image_zip</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">get_file</span><span class="p">(</span><span class="s1">&#39;train2014.zip&#39;</span><span class="p">,</span>
                                      <span class="n">cache_subdir</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">),</span>
                                      <span class="n">origin</span><span class="o">=</span><span class="s1">&#39;http://images.cocodataset.org/zips/train2014.zip&#39;</span><span class="p">,</span>
                                      <span class="n">extract</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">PATH</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">image_zip</span><span class="p">)</span> <span class="o">+</span> <span class="n">image_folder</span>
  <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">image_zip</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
  <span class="n">PATH</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)</span> <span class="o">+</span> <span class="n">image_folder</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Optional:-limit-the-size-of-the-training-set">
<h2>Optional: limit the size of the training set<a class="headerlink" href="#Optional:-limit-the-size-of-the-training-set" title="Enlazar permanentemente con este título"></a></h2>
<p>To speed up training for this tutorial, you’ll use a subset of 30,000 captions and their corresponding images to train your model. Choosing to use more data would result in improved captioning quality.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">annotation_file</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">annotations</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Group all captions together having the same image ID.</span>
<span class="n">image_path_to_caption</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
<span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">annotations</span><span class="p">[</span><span class="s1">&#39;annotations&#39;</span><span class="p">]:</span>
  <span class="n">caption</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&lt;start&gt; </span><span class="si">{</span><span class="n">val</span><span class="p">[</span><span class="s1">&#39;caption&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> &lt;end&gt;&quot;</span>
  <span class="n">image_path</span> <span class="o">=</span> <span class="n">PATH</span> <span class="o">+</span> <span class="s1">&#39;COCO_train2014_&#39;</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">%012d</span><span class="s1">.jpg&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">val</span><span class="p">[</span><span class="s1">&#39;image_id&#39;</span><span class="p">])</span>
  <span class="n">image_path_to_caption</span><span class="p">[</span><span class="n">image_path</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">caption</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">image_paths</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">image_path_to_caption</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">image_paths</span><span class="p">)</span>

<span class="c1"># Select the first 6000 image_paths from the shuffled set.</span>
<span class="c1"># Approximately each image id has 5 captions associated with it, so that will</span>
<span class="c1"># lead to 30,000 examples.</span>
<span class="n">train_image_paths</span> <span class="o">=</span> <span class="n">image_paths</span><span class="p">[:</span><span class="mi">6000</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_image_paths</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">train_captions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">img_name_vector</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">image_path</span> <span class="ow">in</span> <span class="n">train_image_paths</span><span class="p">:</span>
  <span class="n">caption_list</span> <span class="o">=</span> <span class="n">image_path_to_caption</span><span class="p">[</span><span class="n">image_path</span><span class="p">]</span>
  <span class="n">train_captions</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">caption_list</span><span class="p">)</span>
  <span class="n">img_name_vector</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">image_path</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">caption_list</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="n">train_captions</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">img_name_vector</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Preprocess-the-images-using-InceptionV3">
<h2>Preprocess the images using InceptionV3<a class="headerlink" href="#Preprocess-the-images-using-InceptionV3" title="Enlazar permanentemente con este título"></a></h2>
<p>Next, you will use InceptionV3 (which is pretrained on Imagenet) to classify each image. You will extract features from the last convolutional layer.</p>
<p>First, you will convert the images into InceptionV3’s expected format by: * Resizing the image to 299px by 299px * <a class="reference external" href="https://cloud.google.com/tpu/docs/inception-v3-advanced#preprocessing_stage">Preprocess the images</a> using the <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3/preprocess_input">preprocess_input</a> method to normalize the image so that it contains pixels in the range of -1 to 1, which matches the format of the images used to train InceptionV3.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">load_image</span><span class="p">(</span><span class="n">image_path</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">read_file</span><span class="p">(</span><span class="n">image_path</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">decode_jpeg</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">channels</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Resizing</span><span class="p">(</span><span class="mi">299</span><span class="p">,</span> <span class="mi">299</span><span class="p">)(</span><span class="n">img</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">applications</span><span class="o">.</span><span class="n">inception_v3</span><span class="o">.</span><span class="n">preprocess_input</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">img</span><span class="p">,</span> <span class="n">image_path</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Initialize-InceptionV3-and-load-the-pretrained-Imagenet-weights">
<h2>Initialize InceptionV3 and load the pretrained Imagenet weights<a class="headerlink" href="#Initialize-InceptionV3-and-load-the-pretrained-Imagenet-weights" title="Enlazar permanentemente con este título"></a></h2>
<p>Now you’ll create a tf.keras model where the output layer is the last convolutional layer in the InceptionV3 architecture. The shape of the output of this layer is <code class="docutils literal notranslate"><span class="pre">8x8x2048</span></code>. You use the last convolutional layer because you are using attention in this example. You don’t perform this initialization during training because it could become a bottleneck.</p>
<ul class="simple">
<li><p>You forward each image through the network and store the resulting vector in a dictionary (image_name –&gt; feature_vector).</p></li>
<li><p>After all the images are passed through the network, you save the dictionary to disk.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">image_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">applications</span><span class="o">.</span><span class="n">InceptionV3</span><span class="p">(</span><span class="n">include_top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                                <span class="n">weights</span><span class="o">=</span><span class="s1">&#39;imagenet&#39;</span><span class="p">)</span>
<span class="n">new_input</span> <span class="o">=</span> <span class="n">image_model</span><span class="o">.</span><span class="n">input</span>
<span class="n">hidden_layer</span> <span class="o">=</span> <span class="n">image_model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span>

<span class="n">image_features_extract_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">new_input</span><span class="p">,</span> <span class="n">hidden_layer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Caching-the-features-extracted-from-InceptionV3">
<h2>Caching the features extracted from InceptionV3<a class="headerlink" href="#Caching-the-features-extracted-from-InceptionV3" title="Enlazar permanentemente con este título"></a></h2>
<p>You will pre-process each image with InceptionV3 and cache the output to disk. Caching the output in RAM would be faster but also memory intensive, requiring 8 * 8 * 2048 floats per image. At the time of writing, this exceeds the memory limitations of Colab (currently 12GB of memory).</p>
<p>Performance could be improved with a more sophisticated caching strategy (for example, by sharding the images to reduce random access disk I/O), but that would require more code.</p>
<p>The caching will take about 10 minutes to run in Colab with a GPU. If you’d like to see a progress bar, you can:</p>
<ol class="arabic">
<li><p>Install <a class="reference external" href="https://github.com/tqdm/tqdm">tqdm</a>:</p>
<p><code class="docutils literal notranslate"><span class="pre">!pip</span> <span class="pre">install</span> <span class="pre">tqdm</span></code></p>
</li>
<li><p>Import tqdm:</p>
<p><code class="docutils literal notranslate"><span class="pre">from</span> <span class="pre">tqdm</span> <span class="pre">import</span> <span class="pre">tqdm</span></code></p>
</li>
<li><p>Change the following line:</p>
<p><code class="docutils literal notranslate"><span class="pre">for</span> <span class="pre">img,</span> <span class="pre">path</span> <span class="pre">in</span> <span class="pre">image_dataset:</span></code></p>
<p>to:</p>
<p><code class="docutils literal notranslate"><span class="pre">for</span> <span class="pre">img,</span> <span class="pre">path</span> <span class="pre">in</span> <span class="pre">tqdm(image_dataset):</span></code></p>
</li>
</ol>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Get unique images</span>
<span class="n">encode_train</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">img_name_vector</span><span class="p">))</span>

<span class="c1"># Feel free to change batch_size according to your system configuration</span>
<span class="n">image_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span><span class="n">encode_train</span><span class="p">)</span>
<span class="n">image_dataset</span> <span class="o">=</span> <span class="n">image_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
  <span class="n">load_image</span><span class="p">,</span> <span class="n">num_parallel_calls</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>

<span class="k">for</span> <span class="n">img</span><span class="p">,</span> <span class="n">path</span> <span class="ow">in</span> <span class="n">image_dataset</span><span class="p">:</span>
  <span class="n">batch_features</span> <span class="o">=</span> <span class="n">image_features_extract_model</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
  <span class="n">batch_features</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_features</span><span class="p">,</span>
                              <span class="p">(</span><span class="n">batch_features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]))</span>

  <span class="k">for</span> <span class="n">bf</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">batch_features</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
    <span class="n">path_of_feature</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">path_of_feature</span><span class="p">,</span> <span class="n">bf</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Preprocess-and-tokenize-the-captions">
<h2>Preprocess and tokenize the captions<a class="headerlink" href="#Preprocess-and-tokenize-the-captions" title="Enlazar permanentemente con este título"></a></h2>
<p>You will transform the text captions into integer sequences using the <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization">TextVectorization</a> layer, with the following steps:</p>
<ul class="simple">
<li><p>Use <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization#adapt">adapt</a> to iterate over all captions, split the captions into words, and compute a vocabulary of the top 5,000 words (to save memory).</p></li>
<li><p>Tokenize all captions by mapping each word to it’s index in the vocabulary. All output sequences will be padded to length 50.</p></li>
<li><p>Create word-to-index and index-to-word mappings to display results.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">caption_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span><span class="n">train_captions</span><span class="p">)</span>

<span class="c1"># We will override the default standardization of TextVectorization to preserve</span>
<span class="c1"># &quot;&lt;&gt;&quot; characters, so we preserve the tokens for the &lt;start&gt; and &lt;end&gt;.</span>
<span class="k">def</span> <span class="nf">standardize</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
  <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">lower</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">regex_replace</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span>
                                  <span class="sa">r</span><span class="s2">&quot;!</span><span class="se">\&quot;</span><span class="s2">#$%&amp;\(\)\*\+.,-/:;=?@\[</span><span class="se">\\</span><span class="s2">\]^_`{|}~&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>

<span class="c1"># Max word count for a caption.</span>
<span class="n">max_length</span> <span class="o">=</span> <span class="mi">50</span>
<span class="c1"># Use the top 5000 words for a vocabulary.</span>
<span class="n">vocabulary_size</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">TextVectorization</span><span class="p">(</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="n">vocabulary_size</span><span class="p">,</span>
    <span class="n">standardize</span><span class="o">=</span><span class="n">standardize</span><span class="p">,</span>
    <span class="n">output_sequence_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">)</span>
<span class="c1"># Learn the vocabulary from the caption data.</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">adapt</span><span class="p">(</span><span class="n">caption_dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Create the tokenized vectors</span>
<span class="n">cap_vector</span> <span class="o">=</span> <span class="n">caption_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Create mappings for words to indices and indicies to words.</span>
<span class="n">word_to_index</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">StringLookup</span><span class="p">(</span>
    <span class="n">mask_token</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
    <span class="n">vocabulary</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">get_vocabulary</span><span class="p">())</span>
<span class="n">index_to_word</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">StringLookup</span><span class="p">(</span>
    <span class="n">mask_token</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
    <span class="n">vocabulary</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">get_vocabulary</span><span class="p">(),</span>
    <span class="n">invert</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Split-the-data-into-training-and-testing">
<h2>Split the data into training and testing<a class="headerlink" href="#Split-the-data-into-training-and-testing" title="Enlazar permanentemente con este título"></a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">img_to_cap_vector</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
<span class="k">for</span> <span class="n">img</span><span class="p">,</span> <span class="n">cap</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">img_name_vector</span><span class="p">,</span> <span class="n">cap_vector</span><span class="p">):</span>
  <span class="n">img_to_cap_vector</span><span class="p">[</span><span class="n">img</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cap</span><span class="p">)</span>

<span class="c1"># Create training and validation sets using an 80-20 split randomly.</span>
<span class="n">img_keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">img_to_cap_vector</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">img_keys</span><span class="p">)</span>

<span class="n">slice_index</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">img_keys</span><span class="p">)</span><span class="o">*</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">img_name_train_keys</span><span class="p">,</span> <span class="n">img_name_val_keys</span> <span class="o">=</span> <span class="n">img_keys</span><span class="p">[:</span><span class="n">slice_index</span><span class="p">],</span> <span class="n">img_keys</span><span class="p">[</span><span class="n">slice_index</span><span class="p">:]</span>

<span class="n">img_name_train</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">cap_train</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">imgt</span> <span class="ow">in</span> <span class="n">img_name_train_keys</span><span class="p">:</span>
  <span class="n">capt_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">img_to_cap_vector</span><span class="p">[</span><span class="n">imgt</span><span class="p">])</span>
  <span class="n">img_name_train</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">imgt</span><span class="p">]</span> <span class="o">*</span> <span class="n">capt_len</span><span class="p">)</span>
  <span class="n">cap_train</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">img_to_cap_vector</span><span class="p">[</span><span class="n">imgt</span><span class="p">])</span>

<span class="n">img_name_val</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">cap_val</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">imgv</span> <span class="ow">in</span> <span class="n">img_name_val_keys</span><span class="p">:</span>
  <span class="n">capv_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">img_to_cap_vector</span><span class="p">[</span><span class="n">imgv</span><span class="p">])</span>
  <span class="n">img_name_val</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">imgv</span><span class="p">]</span> <span class="o">*</span> <span class="n">capv_len</span><span class="p">)</span>
  <span class="n">cap_val</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">img_to_cap_vector</span><span class="p">[</span><span class="n">imgv</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">len</span><span class="p">(</span><span class="n">img_name_train</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">cap_train</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">img_name_val</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">cap_val</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Create-a-tf.data-dataset-for-training">
<h2>Create a tf.data dataset for training<a class="headerlink" href="#Create-a-tf.data-dataset-for-training" title="Enlazar permanentemente con este título"></a></h2>
<p>Your images and captions are ready! Next, let’s create a <code class="docutils literal notranslate"><span class="pre">tf.data</span></code> dataset to use for training your model.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Feel free to change these parameters according to your system&#39;s configuration</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">BUFFER_SIZE</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">units</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">img_name_train</span><span class="p">)</span> <span class="o">//</span> <span class="n">BATCH_SIZE</span>
<span class="c1"># Shape of the vector extracted from InceptionV3 is (64, 2048)</span>
<span class="c1"># These two variables represent that vector shape</span>
<span class="n">features_shape</span> <span class="o">=</span> <span class="mi">2048</span>
<span class="n">attention_features_shape</span> <span class="o">=</span> <span class="mi">64</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Load the numpy files</span>
<span class="k">def</span> <span class="nf">map_func</span><span class="p">(</span><span class="n">img_name</span><span class="p">,</span> <span class="n">cap</span><span class="p">):</span>
  <span class="n">img_tensor</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">img_name</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">+</span><span class="s1">&#39;.npy&#39;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">img_tensor</span><span class="p">,</span> <span class="n">cap</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">img_name_train</span><span class="p">,</span> <span class="n">cap_train</span><span class="p">))</span>

<span class="c1"># Use map to load the numpy files in parallel</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">item1</span><span class="p">,</span> <span class="n">item2</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">numpy_function</span><span class="p">(</span>
          <span class="n">map_func</span><span class="p">,</span> <span class="p">[</span><span class="n">item1</span><span class="p">,</span> <span class="n">item2</span><span class="p">],</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">]),</span>
          <span class="n">num_parallel_calls</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>

<span class="c1"># Shuffle and batch</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">BUFFER_SIZE</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Model">
<h2>Model<a class="headerlink" href="#Model" title="Enlazar permanentemente con este título"></a></h2>
<p>Fun fact: the decoder below is identical to the one in the example for <a class="reference external" href="https://www.tensorflow.org/text/tutorials/nmt_with_attention">Neural Machine Translation with Attention</a>.</p>
<p>The model architecture is inspired by the <a class="reference external" href="https://arxiv.org/pdf/1502.03044.pdf">Show, Attend and Tell</a> paper.</p>
<ul class="simple">
<li><p>In this example, you extract the features from the lower convolutional layer of InceptionV3 giving us a vector of shape (8, 8, 2048).</p></li>
<li><p>You squash that to a shape of (64, 2048).</p></li>
<li><p>This vector is then passed through the CNN Encoder (which consists of a single Fully connected layer).</p></li>
<li><p>The RNN (here GRU) attends over the image to predict the next word.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">BahdanauAttention</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">units</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BahdanauAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">hidden</span><span class="p">):</span>
    <span class="c1"># features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)</span>

    <span class="c1"># hidden shape == (batch_size, hidden_size)</span>
    <span class="c1"># hidden_with_time_axis shape == (batch_size, 1, hidden_size)</span>
    <span class="n">hidden_with_time_axis</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># attention_hidden_layer shape == (batch_size, 64, units)</span>
    <span class="n">attention_hidden_layer</span> <span class="o">=</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W1</span><span class="p">(</span><span class="n">features</span><span class="p">)</span> <span class="o">+</span>
                                         <span class="bp">self</span><span class="o">.</span><span class="n">W2</span><span class="p">(</span><span class="n">hidden_with_time_axis</span><span class="p">)))</span>

    <span class="c1"># score shape == (batch_size, 64, 1)</span>
    <span class="c1"># This gives you an unnormalized score for each image feature.</span>
    <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">(</span><span class="n">attention_hidden_layer</span><span class="p">)</span>

    <span class="c1"># attention_weights shape == (batch_size, 64, 1)</span>
    <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># context_vector shape after sum == (batch_size, hidden_size)</span>
    <span class="n">context_vector</span> <span class="o">=</span> <span class="n">attention_weights</span> <span class="o">*</span> <span class="n">features</span>
    <span class="n">context_vector</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">context_vector</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">context_vector</span><span class="p">,</span> <span class="n">attention_weights</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">CNN_Encoder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="c1"># Since you have already extracted the features and dumped it</span>
    <span class="c1"># This encoder passes those features through a Fully connected layer</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CNN_Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># shape after fc == (batch_size, 64, embedding_dim)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">RNN_Decoder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">units</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">RNN_Decoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">units</span> <span class="o">=</span> <span class="n">units</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">,</span>
                                   <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                   <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                   <span class="n">recurrent_initializer</span><span class="o">=</span><span class="s1">&#39;glorot_uniform&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">BahdanauAttention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">hidden</span><span class="p">):</span>
    <span class="c1"># defining attention as a separate model</span>
    <span class="n">context_vector</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>

    <span class="c1"># x shape after passing through embedding == (batch_size, 1, embedding_dim)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">context_vector</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">x</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># passing the concatenated vector to the GRU</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># shape == (batch_size, max_length, hidden_size)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

    <span class="c1"># x shape == (batch_size * max_length, hidden_size)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>

    <span class="c1"># output shape == (batch_size * max_length, vocab)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">attention_weights</span>

  <span class="k">def</span> <span class="nf">reset_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">encoder</span> <span class="o">=</span> <span class="n">CNN_Encoder</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">RNN_Decoder</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">units</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">vocabulary_size</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">()</span>
<span class="n">loss_object</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span>
    <span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">pred</span><span class="p">):</span>
  <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
  <span class="n">loss_</span> <span class="o">=</span> <span class="n">loss_object</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>

  <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">loss_</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
  <span class="n">loss_</span> <span class="o">*=</span> <span class="n">mask</span>

  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss_</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Checkpoint">
<h2>Checkpoint<a class="headerlink" href="#Checkpoint" title="Enlazar permanentemente con este título"></a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">checkpoint_path</span> <span class="o">=</span> <span class="s2">&quot;./checkpoints/train&quot;</span>
<span class="n">ckpt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Checkpoint</span><span class="p">(</span><span class="n">encoder</span><span class="o">=</span><span class="n">encoder</span><span class="p">,</span>
                           <span class="n">decoder</span><span class="o">=</span><span class="n">decoder</span><span class="p">,</span>
                           <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>
<span class="n">ckpt_manager</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">CheckpointManager</span><span class="p">(</span><span class="n">ckpt</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">max_to_keep</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">start_epoch</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">if</span> <span class="n">ckpt_manager</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">:</span>
  <span class="n">start_epoch</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">ckpt_manager</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
  <span class="c1"># restoring the latest checkpoint in checkpoint_path</span>
  <span class="n">ckpt</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">ckpt_manager</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Training">
<h2>Training<a class="headerlink" href="#Training" title="Enlazar permanentemente con este título"></a></h2>
<ul class="simple">
<li><p>You extract the features stored in the respective <code class="docutils literal notranslate"><span class="pre">.npy</span></code> files and then pass those features through the encoder.</p></li>
<li><p>The encoder output, hidden state(initialized to 0) and the decoder input (which is the start token) is passed to the decoder.</p></li>
<li><p>The decoder returns the predictions and the decoder hidden state.</p></li>
<li><p>The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.</p></li>
<li><p>Use teacher forcing to decide the next input to the decoder.</p></li>
<li><p>Teacher forcing is the technique where the target word is passed as the next input to the decoder.</p></li>
<li><p>The final step is to calculate the gradients and apply it to the optimizer and backpropagate.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># adding this in a separate cell because if you run the training cell</span>
<span class="c1"># many times, the loss_plot array will be reset</span>
<span class="n">loss_plot</span> <span class="o">=</span> <span class="p">[]</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>

  <span class="c1"># initializing the hidden state for each batch</span>
  <span class="c1"># because the captions are not related from image to image</span>
  <span class="n">hidden</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">reset_state</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

  <span class="n">dec_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">([</span><span class="n">word_to_index</span><span class="p">(</span><span class="s1">&#39;&lt;start&gt;&#39;</span><span class="p">)]</span> <span class="o">*</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>

  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
      <span class="n">features</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">)</span>

      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
          <span class="c1"># passing the features through the decoder</span>
          <span class="n">predictions</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">dec_input</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>

          <span class="n">loss</span> <span class="o">+=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">target</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">predictions</span><span class="p">)</span>

          <span class="c1"># using teacher forcing</span>
          <span class="n">dec_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">target</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>

  <span class="n">total_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss</span> <span class="o">/</span> <span class="nb">int</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

  <span class="n">trainable_variables</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">trainable_variables</span> <span class="o">+</span> <span class="n">decoder</span><span class="o">.</span><span class="n">trainable_variables</span>

  <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">trainable_variables</span><span class="p">)</span>

  <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">trainable_variables</span><span class="p">))</span>

  <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">total_loss</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">20</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_epoch</span><span class="p">,</span> <span class="n">EPOCHS</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">img_tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
        <span class="n">batch_loss</span><span class="p">,</span> <span class="n">t_loss</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">t_loss</span>

        <span class="k">if</span> <span class="n">batch</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">average_batch_loss</span> <span class="o">=</span> <span class="n">batch_loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">/</span><span class="nb">int</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1"> Batch </span><span class="si">{</span><span class="n">batch</span><span class="si">}</span><span class="s1"> Loss </span><span class="si">{</span><span class="n">average_batch_loss</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="c1"># storing the epoch end loss value to plot later</span>
    <span class="n">loss_plot</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_loss</span> <span class="o">/</span> <span class="n">num_steps</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">ckpt_manager</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1"> Loss </span><span class="si">{</span><span class="n">total_loss</span><span class="o">/</span><span class="n">num_steps</span><span class="si">:</span><span class="s1">.6f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Time taken for 1 epoch </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1"> sec</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_plot</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epochs&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Loss Plot&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Caption!">
<h2>Caption!<a class="headerlink" href="#Caption!" title="Enlazar permanentemente con este título"></a></h2>
<ul class="simple">
<li><p>The evaluate function is similar to the training loop, except you don’t use teacher forcing here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.</p></li>
<li><p>Stop predicting when the model predicts the end token.</p></li>
<li><p>And store the attention weights for every time step.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">image</span><span class="p">):</span>
    <span class="n">attention_plot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">max_length</span><span class="p">,</span> <span class="n">attention_features_shape</span><span class="p">))</span>

    <span class="n">hidden</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">reset_state</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">temp_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">load_image</span><span class="p">(</span><span class="n">image</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">img_tensor_val</span> <span class="o">=</span> <span class="n">image_features_extract_model</span><span class="p">(</span><span class="n">temp_input</span><span class="p">)</span>
    <span class="n">img_tensor_val</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img_tensor_val</span><span class="p">,</span> <span class="p">(</span><span class="n">img_tensor_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                                 <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                                                 <span class="n">img_tensor_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]))</span>

    <span class="n">features</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">img_tensor_val</span><span class="p">)</span>

    <span class="n">dec_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">([</span><span class="n">word_to_index</span><span class="p">(</span><span class="s1">&#39;&lt;start&gt;&#39;</span><span class="p">)],</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_length</span><span class="p">):</span>
        <span class="n">predictions</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">dec_input</span><span class="p">,</span>
                                                         <span class="n">features</span><span class="p">,</span>
                                                         <span class="n">hidden</span><span class="p">)</span>

        <span class="n">attention_plot</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="n">predicted_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">categorical</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">predicted_word</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">as_text</span><span class="p">(</span><span class="n">index_to_word</span><span class="p">(</span><span class="n">predicted_id</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">predicted_word</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">predicted_word</span> <span class="o">==</span> <span class="s1">&#39;&lt;end&gt;&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">result</span><span class="p">,</span> <span class="n">attention_plot</span>

        <span class="n">dec_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">([</span><span class="n">predicted_id</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">attention_plot</span> <span class="o">=</span> <span class="n">attention_plot</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">),</span> <span class="p">:]</span>
    <span class="k">return</span> <span class="n">result</span><span class="p">,</span> <span class="n">attention_plot</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">plot_attention</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">attention_plot</span><span class="p">):</span>
    <span class="n">temp_image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">image</span><span class="p">))</span>

    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

    <span class="n">len_result</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">len_result</span><span class="p">):</span>
        <span class="n">temp_att</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">attention_plot</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
        <span class="n">grid_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">len_result</span><span class="o">/</span><span class="mi">2</span><span class="p">)),</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">grid_size</span><span class="p">,</span> <span class="n">grid_size</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">temp_image</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">temp_att</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="n">img</span><span class="o">.</span><span class="n">get_extent</span><span class="p">())</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># captions on the validation set</span>
<span class="n">rid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">img_name_val</span><span class="p">))</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">img_name_val</span><span class="p">[</span><span class="n">rid</span><span class="p">]</span>
<span class="n">real_caption</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">as_text</span><span class="p">(</span><span class="n">index_to_word</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
                         <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">cap_val</span><span class="p">[</span><span class="n">rid</span><span class="p">]</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
<span class="n">result</span><span class="p">,</span> <span class="n">attention_plot</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Real Caption:&#39;</span><span class="p">,</span> <span class="n">real_caption</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Prediction Caption:&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">result</span><span class="p">))</span>
<span class="n">plot_attention</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">attention_plot</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Try-it-on-your-own-images">
<h2>Try it on your own images<a class="headerlink" href="#Try-it-on-your-own-images" title="Enlazar permanentemente con este título"></a></h2>
<p>For fun, below you’re provided a method you can use to caption your own images with the model you’ve just trained. Keep in mind, it was trained on a relatively small amount of data, and your images may be different from the training data (so be prepared for weird results!)</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">image_url</span> <span class="o">=</span> <span class="s1">&#39;https://tensorflow.org/images/surf.jpg&#39;</span>
<span class="n">image_extension</span> <span class="o">=</span> <span class="n">image_url</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">:]</span>
<span class="n">image_path</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">get_file</span><span class="p">(</span><span class="s1">&#39;image&#39;</span><span class="o">+</span><span class="n">image_extension</span><span class="p">,</span> <span class="n">origin</span><span class="o">=</span><span class="n">image_url</span><span class="p">)</span>

<span class="n">result</span><span class="p">,</span> <span class="n">attention_plot</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">image_path</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Prediction Caption:&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">result</span><span class="p">))</span>
<span class="n">plot_attention</span><span class="p">(</span><span class="n">image_path</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">attention_plot</span><span class="p">)</span>
<span class="c1"># opening the image</span>
<span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Next-steps">
<h1>Next steps<a class="headerlink" href="#Next-steps" title="Enlazar permanentemente con este título"></a></h1>
<p>Congrats! You’ve just trained an image captioning model with attention. Next, take a look at this example <a class="reference external" href="https://www.tensorflow.org/text/tutorials/nmt_with_attention">Neural Machine Translation with Attention</a>. It uses a similar architecture to translate between Spanish and English sentences. You can also experiment with training the code in this notebook on a different dataset.</p>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Derechos de autor 2019-2021, Juan D. Velasquez.</p>
  </div>

  Compilado con <a href="https://www.sphinx-doc.org/">Sphinx</a> usando un
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">tema</a>
    proporcionado por <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXX-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-XXXXXXX-1', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>